{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":127612,"sourceType":"datasetVersion","datasetId":64826},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171,"modelId":3533}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:00:06.873033Z","iopub.execute_input":"2024-11-01T18:00:06.873400Z","iopub.status.idle":"2024-11-01T18:00:07.288374Z","shell.execute_reply.started":"2024-11-01T18:00:06.873364Z","shell.execute_reply":"2024-11-01T18:00:07.287182Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/medicaltranscriptions/mtsamples.csv\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/config.json\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/tokenizer.json\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/metadata.json\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/model.weights.h5\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/assets/tokenizer/vocabulary.spm\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"![Gemma+lora](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp)","metadata":{}},{"cell_type":"markdown","source":"## Setup\n\n### Get access to Gemma\n\nStep 1: The Gemma setup instructions show how to do the following:\n\nGemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n\n* Sign in or register at [kaggle.com](https://www.kaggle.com/)\n* Open the [Gemma model card](https://www.kaggle.com/models/google/gemma) and select \"Request Access\"\n* Complete the consent form and accept the terms and conditions","metadata":{}},{"cell_type":"markdown","source":"### Install dependencies\nInstall Keras, KerasNLP, and other dependencies.","metadata":{}},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3\n# !pip install -U datasets huggingface_hub fsspec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:00:07.290457Z","iopub.execute_input":"2024-11-01T18:00:07.290889Z","iopub.status.idle":"2024-11-01T18:00:38.614805Z","shell.execute_reply.started":"2024-11-01T18:00:07.290838Z","shell.execute_reply":"2024-11-01T18:00:38.613365Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Select a backend\nKeras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. Using Keras 3, we can run workflows on one of three backends: TensorFlow, JAX, or PyTorch.\n\nFor this, configure the backend for Pytorch.","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:00:38.616411Z","iopub.execute_input":"2024-11-01T18:00:38.616769Z","iopub.status.idle":"2024-11-01T18:00:38.623246Z","shell.execute_reply.started":"2024-11-01T18:00:38.616732Z","shell.execute_reply":"2024-11-01T18:00:38.621902Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Import packages\nImport Keras and KerasNLP.","metadata":{}},{"cell_type":"code","source":"import keras\nimport keras_nlp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:00:38.624924Z","iopub.execute_input":"2024-11-01T18:00:38.625296Z","iopub.status.idle":"2024-11-01T18:00:52.127165Z","shell.execute_reply.started":"2024-11-01T18:00:38.625260Z","shell.execute_reply":"2024-11-01T18:00:52.126198Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Dataset\n\nThis [\"MT Samples\"](https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions) dataset provides a solution to the challenge of acquiring medical data due to HIPAA privacy regulations by offering sample medical transcriptions from various specialties. Comprising 5,000 rows, it enables the classification of medical specialties based on the transcription text. The data was sourced from [\"mtsamples.com\"](https://mtsamples.com/), and it serves as a valuable resource for researchers and developers aiming to enhance their understanding of medical language processing and improve classification models in healthcare applications.\n\nThe [\"Medical Transcripts\"](https://huggingface.co/datasets/DataFog/medical-transcription-instruct) dataset contains 38,924 samples of instruct-input-output data tailored for training instruction-following models in the medical field. It is sourced from original medical transcriptions and formatted in CSV, featuring instruction-output pairs. Each row includes an instruction, task output, transcription text, description, medical specialty, sample name, and both original and derived keywords. Additional columns provide transcription length, normalized length, and a complexity score. Tasks include identifying medical specialties, summarizing transcriptions, extracting keywords, assessing text complexity, and suggesting follow-up questions. The dataset focuses on the medical domain, supporting diverse instruction-following tasks in healthcare.\n","metadata":{}},{"cell_type":"markdown","source":"### Load Dataset","metadata":{}},{"cell_type":"code","source":"import pyarrow.parquet as pq\nimport pandas as pd\n\n# Step 1: Read Parquet file\ndf = pd.read_csv('/kaggle/input/medicaltranscriptions/mtsamples.csv')\n\n# df = pd.read_csv(\"hf://datasets/DataFog/medical-transcription-instruct/datafog-medical-transcription-instruct.csv\")\n\n# # Step 2: Convert table to DataFrame\n# df = table.to_pandas()\n\n# Step 3: Access data\ndf.head()  # Display first few rows of the DataFrame\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:00:52.130423Z","iopub.execute_input":"2024-11-01T18:00:52.131223Z","iopub.status.idle":"2024-11-01T18:00:52.608505Z","shell.execute_reply.started":"2024-11-01T18:00:52.131168Z","shell.execute_reply":"2024-11-01T18:00:52.607412Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                        description  \\\n0           0   A 23-year-old white female presents with comp...   \n1           1           Consult for laparoscopic gastric bypass.   \n2           2           Consult for laparoscopic gastric bypass.   \n3           3                             2-D M-Mode. Doppler.     \n4           4                                 2-D Echocardiogram   \n\n             medical_specialty                                sample_name  \\\n0         Allergy / Immunology                         Allergic Rhinitis    \n1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n\n                                       transcription  \\\n0  SUBJECTIVE:,  This 23-year-old white female pr...   \n1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n3  2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n4  1.  The left ventricular cavity size and wall ...   \n\n                                            keywords  \n0  allergy / immunology, allergic rhinitis, aller...  \n1  bariatrics, laparoscopic gastric bypass, weigh...  \n2  bariatrics, laparoscopic gastric bypass, heart...  \n3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n4  cardiovascular / pulmonary, 2-d, doppler, echo...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>description</th>\n      <th>medical_specialty</th>\n      <th>sample_name</th>\n      <th>transcription</th>\n      <th>keywords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>A 23-year-old white female presents with comp...</td>\n      <td>Allergy / Immunology</td>\n      <td>Allergic Rhinitis</td>\n      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n      <td>allergy / immunology, allergic rhinitis, aller...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Consult for laparoscopic gastric bypass.</td>\n      <td>Bariatrics</td>\n      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Consult for laparoscopic gastric bypass.</td>\n      <td>Bariatrics</td>\n      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n      <td>bariatrics, laparoscopic gastric bypass, heart...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2-D M-Mode. Doppler.</td>\n      <td>Cardiovascular / Pulmonary</td>\n      <td>2-D Echocardiogram - 1</td>\n      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2-D Echocardiogram</td>\n      <td>Cardiovascular / Pulmonary</td>\n      <td>2-D Echocardiogram - 2</td>\n      <td>1.  The left ventricular cavity size and wall ...</td>\n      <td>cardiovascular / pulmonary, 2-d, doppler, echo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n\n# train, test = train_test_split(df, test_size=0.8)\n\n# # Further split the train set to keep exactly 27,000 in the train set\n# train_final, extra_for_test = train_test_split(train, test_size=1677/len(train), random_state=42)\n\n# # Use pd.concat to combine the extra_for_test with the test set\n# test_final = pd.concat([test, extra_for_test])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:00:52.609828Z","iopub.execute_input":"2024-11-01T18:00:52.610445Z","iopub.status.idle":"2024-11-01T18:00:52.614740Z","shell.execute_reply.started":"2024-11-01T18:00:52.610404Z","shell.execute_reply":"2024-11-01T18:00:52.613926Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Initialize an empty list to store the formatted transcriptions and keywords\nformatted_data = []\n\n# Iterate over each row of the DataFrame\nfor index, row in df.iterrows():\n    # Extract messages\n    transcription = row['transcription']\n    keywords = row['keywords']\n#     instructions = row['instruction']\n#     task_output = row['task_output']\n    description = row['description']\n    medical_specialty = row['medical_specialty']\n    \n    # Create the formatted string\n#     formatted_string = f'Transcription:\\n{transcription}\\n\\nKeywords:\\n{keywords}\\n\\nMedical Specialty:{medical_specialty}\\n\\nInstruction:{instructions}\\n\\nOutput:{task_output}'\n    formatted_string = f'Transcription:\\n{transcription}\\n\\nKeywords:\\n{keywords}\\n\\nMedical Specialty:{medical_specialty}\\n\\nDescription: {description}'\n    \n    # Append the formatted string to the list\n    formatted_data.append(formatted_string)\n\n# Print the formatted data\nfor item in formatted_data:\n    print(item)\n    break\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:00:52.616098Z","iopub.execute_input":"2024-11-01T18:00:52.616392Z","iopub.status.idle":"2024-11-01T18:00:53.016239Z","shell.execute_reply.started":"2024-11-01T18:00:52.616361Z","shell.execute_reply":"2024-11-01T18:00:53.015197Z"}},"outputs":[{"name":"stdout","text":"Transcription:\nSUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.\n\nKeywords:\nallergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,\n\nMedical Specialty: Allergy / Immunology\n\nDescription:  A 23-year-old white female presents with complaint of allergies.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# formatted_data = formatted_data[:200]\nprint(len(formatted_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:00:53.017583Z","iopub.execute_input":"2024-11-01T18:00:53.017960Z","iopub.status.idle":"2024-11-01T18:00:53.023250Z","shell.execute_reply.started":"2024-11-01T18:00:53.017923Z","shell.execute_reply":"2024-11-01T18:00:53.022253Z"}},"outputs":[{"name":"stdout","text":"4999\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Load Model\n\nKerasNLP provides implementations of many popular model architectures. In this, the model is created using GemmaCausalLM, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.\n\nCreate the model using the from_preset method:","metadata":{}},{"cell_type":"code","source":"# import shutil\n\n# # Zip the file so it can be downloaded\n# shutil.make_archive('/kaggle/working/gemma-medtr-2b-v2', 'zip', '/kaggle/working/gemma-medtr-2b-v2')\n\n# # The file will be available for download from the output section of your notebook\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:00:53.024530Z","iopub.execute_input":"2024-11-01T18:00:53.024836Z","iopub.status.idle":"2024-11-01T18:00:53.034026Z","shell.execute_reply.started":"2024-11-01T18:00:53.024805Z","shell.execute_reply":"2024-11-01T18:00:53.032922Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"gemma_model = keras_nlp.models.GemmaCausalLM.from_preset('/kaggle/input/gemma/keras/gemma_instruct_2b_en/2')\ngemma_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:00:53.035295Z","iopub.execute_input":"2024-11-01T18:00:53.035615Z","iopub.status.idle":"2024-11-01T18:01:35.844234Z","shell.execute_reply.started":"2024-11-01T18:00:53.035581Z","shell.execute_reply":"2024-11-01T18:01:35.843085Z"}},"outputs":[{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"The from_preset method instantiates the model from a preset architecture and weights. In the code above, the string \"gemma_2b_en\" specifies the preset architecture — a Gemma model with 2 billion parameters.","metadata":{}},{"cell_type":"markdown","source":"### Inference before fine tuning\nIn this section, we will query the model with various prompts to see how it responds.","metadata":{}},{"cell_type":"markdown","source":"#### laparoscopic gastric bypass Prompt\nQuery the model for laparoscopic gastric bypass transcription.","metadata":{}},{"cell_type":"code","source":"prompt = \"tell me something about treating laparoscopic gastric bypass with an example\"\nprint(gemma_model.generate(prompt, max_length=256))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:01:35.845701Z","iopub.execute_input":"2024-11-01T18:01:35.846041Z","iopub.status.idle":"2024-11-01T18:01:51.334299Z","shell.execute_reply.started":"2024-11-01T18:01:35.846005Z","shell.execute_reply":"2024-11-01T18:01:51.333252Z"}},"outputs":[{"name":"stdout","text":"tell me something about treating laparoscopic gastric bypass with an example.\n\nLaparoscopic gastric bypass (LGB) is a surgical procedure that can treat obesity and type 2 diabetes. It involves creating a small stomach pouch and connecting it directly to the small intestine. This bypasses the majority of the stomach and upper small intestine, reducing the amount of food the patient can eat and promoting weight loss.\n\n**Example:**\n\nA 40-year-old woman with a body mass index (BMI) of 40 and a history of obesity and type 2 diabetes is considering LGB. Her doctor discusses the procedure with her and explains the benefits and risks. The woman decides to proceed with LGB.\n\n**Steps involved in LGB:**\n\n1. An incision is made in the abdomen.\n2. The stomach is divided into two parts: the upper part and the lower part.\n3. The lower part is removed and connected directly to the small intestine.\n4. The upper part of the stomach is closed with stitches.\n\n**Benefits of LGB:**\n\n* Rapid weight loss\n* Improved blood sugar control\n* Lowered blood pressure\n* Reduced cholesterol levels\n* Improved insulin sensitivity\n\n**Risks associated with LGB:**\n\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### LoRA Fine-tuning","metadata":{}},{"cell_type":"markdown","source":"To get better responses from the model, fine-tune the model with Low Rank Adaptation (LoRA) using the mt samples dataset.\n\nLoRA (Low-Rank Adaptation of Large Language Models) has gained popularity as a lightweight training method that slashes the number of parameters to train. Instead of adjusting every single weight in the model, LoRA adds a smaller set of new weights and focuses training solely on them. This approach speeds up training, saves memory, and generates smaller model sizes (just a few 100 MBs), making them simpler to store and share. To speed up training more, it can also be combined with other training techniques like dreambooth.\n\nHaving a higher rank allows for more detailed adjustments, which can enhance precision but also increases the number of parameters to train. On the other hand, a lower rank reduces computational burden but might result in less precise adaptation.\n\nFor this, a LoRA rank of 4 is utilized. It's recommended to start with a relatively small rank, like 4, 8, or 16, for computational efficiency during experimentation. Train model with rank=4 initially and assess its performance improvement on specific task. As we progress, we can gradually increase the rank in subsequent experiments to observe if it leads to further enhancements in performance.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 8.\ngemma_model.backbone.enable_lora(rank=8)\ngemma_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:01:51.335765Z","iopub.execute_input":"2024-11-01T18:01:51.336167Z","iopub.status.idle":"2024-11-01T18:01:51.631411Z","shell.execute_reply.started":"2024-11-01T18:01:51.336131Z","shell.execute_reply":"2024-11-01T18:01:51.630175Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,508,900,352\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,508,900,352</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,508,900,352\u001b[0m (9.35 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,508,900,352</span> (9.35 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,727,936\u001b[0m (10.41 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,727,936</span> (10.41 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"> Note that enabling LoRA reduces the number of trainable parameters significantly (from 2.5 billion to 1.3 million).","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameters for lora\nWe are defining only rank, rest will be default hyperparameters.\n\nRank (r): 4\nWe opted for a rank of 4 in our decomposition matrices to maintain efficiency gains while still achieving solid performance. Although testing higher ranks, like 8 or 16, showed minimal performance improvements, although it is recommended to sticking with 8 if harware compatibility is there it helps to keep checkpoint sizes manageable without sacrificing too much accuracy.\n\nAlpha (lora_alpha):\nAlpha scales the learned weights. Based on existing [literature](https://arxiv.org/pdf/2308.07317v1.pdf) and recommendations from the original [LoRA paper](https://arxiv.org/abs/2106.09685). Keeping Alpha fixed rather than treating it as a tunable hyperparameter is a common practice in the LLM community and suggested value is 16.\n\nTarget Modules: All Dense Layers\nWhile the original LoRA paper focused on fine-tuning only the \"Q\" and \"V\" attention matrices, subsequent research suggested that targeting [additional layers](https://arxiv.org/pdf/2110.04366.pdf), or even [all layers](https://arxiv.org/abs/2305.14314), could yield better results. [At backend](https://github.com/keras-team/keras-nlp/blob/v0.8.1/keras_nlp/models/backbone.py#L156) code targets the \"query_dense\" and \"value_dense\" layers in the attention layers for enabling LoRA.\n\nBase Learning Rate: 1e-4\nBase learning rate of 1e-4 has become the standard for fine-tuning LLMs with LoRA. Although [some](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2) have encountered occasional training loss instabilities, lowering the learning rate to values like 3e-5 helped them stabilize the process.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\ntorch.cuda.empty_cache()\nnp.asarray(formatted_data).shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:01:51.632664Z","iopub.execute_input":"2024-11-01T18:01:51.633008Z","iopub.status.idle":"2024-11-01T18:01:55.716562Z","shell.execute_reply.started":"2024-11-01T18:01:51.632972Z","shell.execute_reply":"2024-11-01T18:01:55.715603Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(4999,)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage).\ngemma_model.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_model.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_model.fit(formatted_data, epochs=10, batch_size=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-01T18:01:55.720166Z","iopub.execute_input":"2024-11-01T18:01:55.720514Z","iopub.status.idle":"2024-11-02T03:58:48.131665Z","shell.execute_reply.started":"2024-11-01T18:01:55.720479Z","shell.execute_reply":"2024-11-02T03:58:48.130668Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m4999/4999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3607s\u001b[0m 717ms/step - loss: 2.1163 - sparse_categorical_accuracy: 0.5334\nEpoch 2/10\n\u001b[1m4999/4999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3587s\u001b[0m 715ms/step - loss: 1.8298 - sparse_categorical_accuracy: 0.5736\nEpoch 3/10\n\u001b[1m4999/4999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3577s\u001b[0m 715ms/step - loss: 1.7495 - sparse_categorical_accuracy: 0.5880\nEpoch 4/10\n\u001b[1m4999/4999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3577s\u001b[0m 715ms/step - loss: 1.6883 - sparse_categorical_accuracy: 0.5969\nEpoch 5/10\n\u001b[1m4999/4999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3577s\u001b[0m 715ms/step - loss: 1.6345 - sparse_categorical_accuracy: 0.6066\nEpoch 6/10\n\u001b[1m4999/4999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3577s\u001b[0m 715ms/step - loss: 1.5858 - sparse_categorical_accuracy: 0.6164\nEpoch 7/10\n\u001b[1m4999/4999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3577s\u001b[0m 715ms/step - loss: 1.5384 - sparse_categorical_accuracy: 0.6257\nEpoch 8/10\n\u001b[1m4999/4999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3577s\u001b[0m 715ms/step - loss: 1.4917 - sparse_categorical_accuracy: 0.6346\nEpoch 9/10\n\u001b[1m4999/4999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3577s\u001b[0m 715ms/step - loss: 1.4443 - sparse_categorical_accuracy: 0.6439\nEpoch 10/10\n\u001b[1m4999/4999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3577s\u001b[0m 715ms/step - loss: 1.3961 - sparse_categorical_accuracy: 0.6539\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7809e4475cf0>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"!pip install --upgrade kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T03:58:48.133128Z","iopub.execute_input":"2024-11-02T03:58:48.133865Z","iopub.status.idle":"2024-11-02T03:59:01.224630Z","shell.execute_reply.started":"2024-11-02T03:58:48.133797Z","shell.execute_reply":"2024-11-02T03:59:01.223318Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: kagglehub in /opt/conda/lib/python3.10/site-packages (0.3.1)\nCollecting kagglehub\n  Downloading kagglehub-0.3.3-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from kagglehub) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kagglehub) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kagglehub) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->kagglehub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (2024.8.30)\nDownloading kagglehub-0.3.3-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: kagglehub\n  Attempting uninstall: kagglehub\n    Found existing installation: kagglehub 0.3.1\n    Uninstalling kagglehub-0.3.1:\n      Successfully uninstalled kagglehub-0.3.1\nSuccessfully installed kagglehub-0.3.3\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"gemma_model.save_to_preset(\"./gemma_instruct_medtr_2b_v1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T03:59:01.226540Z","iopub.execute_input":"2024-11-02T03:59:01.226953Z","iopub.status.idle":"2024-11-02T03:59:31.214291Z","shell.execute_reply.started":"2024-11-02T03:59:01.226915Z","shell.execute_reply":"2024-11-02T03:59:31.213329Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"kaggle_uri = \"kaggle://harishiker99/gemma_instruct_medtr_2b/keras/gemma_instruct_medtr_2b\"\nkeras_nlp.upload_preset(kaggle_uri, \"/kaggle/working/gemma_medtr_2b_v6\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T04:13:59.762912Z","iopub.execute_input":"2024-11-02T04:13:59.763625Z","iopub.status.idle":"2024-11-02T04:15:59.613664Z","shell.execute_reply.started":"2024-11-02T04:13:59.763541Z","shell.execute_reply":"2024-11-02T04:15:59.612692Z"}},"outputs":[{"name":"stdout","text":"Uploading Model https://www.kaggle.com/models/harishiker99/gemma_instruct_medtr_2b/keras/gemma_instruct_medtr_2b ...\nModel 'gemma_instruct_medtr_2b' does not exist or access is forbidden for user 'harishiker99'. Creating or handling Model...\nWarning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\nModel 'gemma_instruct_medtr_2b' Created.\nStarting upload for file /kaggle/working/gemma_medtr_2b_v6/task.json\nWarning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|██████████| 2.98k/2.98k [00:00<00:00, 17.4kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/gemma_medtr_2b_v6/task.json (3KB)\nStarting upload for file /kaggle/working/gemma_medtr_2b_v6/preprocessor.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|██████████| 1.41k/1.41k [00:00<00:00, 6.10kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/gemma_medtr_2b_v6/preprocessor.json (1KB)\nStarting upload for file /kaggle/working/gemma_medtr_2b_v6/config.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|██████████| 785/785 [00:00<00:00, 4.23kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/gemma_medtr_2b_v6/config.json (785B)\nStarting upload for file /kaggle/working/gemma_medtr_2b_v6/metadata.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|██████████| 143/143 [00:00<00:00, 815B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/gemma_medtr_2b_v6/metadata.json (143B)\nStarting upload for file /kaggle/working/gemma_medtr_2b_v6/model.weights.h5\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|██████████| 10.0G/10.0G [01:54<00:00, 87.7MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/gemma_medtr_2b_v6/model.weights.h5 (9GB)\nStarting upload for file /kaggle/working/gemma_medtr_2b_v6/tokenizer.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|██████████| 591/591 [00:00<00:00, 3.47kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/gemma_medtr_2b_v6/tokenizer.json (591B)\nStarting upload for file /kaggle/working/gemma_medtr_2b_v6/assets/tokenizer/vocabulary.spm\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|██████████| 4.24M/4.24M [00:00<00:00, 18.9MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/gemma_medtr_2b_v6/assets/tokenizer/vocabulary.spm (4MB)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\nYour model instance has been created.\nFiles are being processed...\nSee at: https://www.kaggle.com/models/harishiker99/gemma_instruct_medtr_2b/keras/gemma_instruct_medtr_2b\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"hf_username = \"harishnair04\"\nmodel_variant_name = \"gemma_instruct_medtr_2b\"\n\n\nuri = f\"hf://{hf_username}/{model_variant_name}\"\nprint(uri)\n\n\nimport huggingface_hub\nhuggingface_hub.login(token=\"HF_KEY\",add_to_git_credential=True)\n\nkeras_nlp.upload_preset(uri, \"/kaggle/working/gemma_medtr_2b_v6\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T04:16:04.990654Z","iopub.execute_input":"2024-11-02T04:16:04.991080Z","iopub.status.idle":"2024-11-02T04:20:02.789169Z","shell.execute_reply.started":"2024-11-02T04:16:04.991043Z","shell.execute_reply":"2024-11-02T04:20:02.788219Z"}},"outputs":[{"name":"stdout","text":"hf://harishnair04/gemma_instruct_medtr_2b\nToken is valid (permission: fineGrained).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocabulary.spm:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10831d3c978d4c09ba6a386c72ea7686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ef49741c6440b7abe3974192827d9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.weights.h5:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93827b36c8054661881a671c2f0fd84a"}},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"### Inference after fine-tuning","metadata":{}},{"cell_type":"markdown","source":"We're training the model on only a small part of the dataset for multiple epoch and with a low LoRA rank setting. If improved responses are required from the fine-tuned model, we might want to try out the following:\n\n- Expanding the size of the dataset used for fine-tuning.\n- Adjusting the LoRA rank to a higher value.\n- Increasing the number of training steps (epochs).\n- Tweaking the hyperparameter values like learning_rate and weight_decay.","metadata":{}},{"cell_type":"code","source":"prompt=\"tell me something about treating laparoscopic gastric bypass with an example\"\nprint(gemma_model.generate(prompt, max_length=256))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T04:05:33.705226Z","iopub.execute_input":"2024-11-02T04:05:33.705584Z","iopub.status.idle":"2024-11-02T04:05:50.513959Z","shell.execute_reply.started":"2024-11-02T04:05:33.705546Z","shell.execute_reply":"2024-11-02T04:05:50.512804Z"}},"outputs":[{"name":"stdout","text":"tell me something about treating laparoscopic gastric bypass with an example of a patient.\n\n**Example:**\n\n**Patient:** John Smith, a 50-year-old man, has been diagnosed with obesity-related high blood pressure, type 2 diabetes, and high cholesterol. He has been struggling with weight loss for several years and has been frustrated with his weight fluctuating and not being able to achieve his weight loss goals.\n\n**Laparoscopic Gastric Bypass Surgery:**\n\nJohn underwent a laparoscopic gastric bypass on March 15, 2015. The procedure was performed by Dr. Jane Doe, a board-certified general surgeon, and a member of the American Society of General Surgeons.\n\n**Benefits of Laparoscopic Gastric Bypass:**\n\n* Rapid recovery with minimal pain and discomfort.\n* Faster return to normal activities.\n* Lower risk of complications.\n* Improved weight loss.\n* Lowered blood pressure.\n* Lowered blood sugar.\n* Lowered cholesterol.\n* Improved overall health.\n\n**Benefits of Laparoscopic Roux-En-Y Gastric Bypass:**\n\n* Permanent weight loss.\n* Lower risk of complications.\n* Improved quality of life.\n* Improved long-term health\n","output_type":"stream"}],"execution_count":19}]}