{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":127612,"sourceType":"datasetVersion","datasetId":64826},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171,"modelId":3533}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"![Gemma+lora](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp)","metadata":{}},{"cell_type":"markdown","source":"## Setup\n\n### Get access to Gemma\n\nStep 1: The Gemma setup instructions show how to do the following:\n\nGemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n\n* Sign in or register at [kaggle.com](https://www.kaggle.com/)\n* Open the [Gemma model card](https://www.kaggle.com/models/google/gemma) and select \"Request Access\"\n* Complete the consent form and accept the terms and conditions","metadata":{}},{"cell_type":"markdown","source":"### Install dependencies\nInstall Keras, KerasNLP, and other dependencies.","metadata":{}},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3\n# !pip install -U datasets huggingface_hub fsspec","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Select a backend\nKeras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. Using Keras 3, we can run workflows on one of three backends: TensorFlow, JAX, or PyTorch.\n\nFor this, configure the backend for Pytorch.","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Import packages\nImport Keras and KerasNLP.","metadata":{}},{"cell_type":"code","source":"import keras\nimport keras_nlp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dataset\n\nThis [\"MT Samples\"](https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions) dataset provides a solution to the challenge of acquiring medical data due to HIPAA privacy regulations by offering sample medical transcriptions from various specialties. Comprising 5,000 rows, it enables the classification of medical specialties based on the transcription text. The data was sourced from mtsamples.com, and it serves as a valuable resource for researchers and developers aiming to enhance their understanding of medical language processing and improve classification models in healthcare applications.\n\nThe [\"Medical Transcripts\"](https://huggingface.co/datasets/DataFog/medical-transcription-instruct) dataset contains 38,924 samples of instruct-input-output data tailored for training instruction-following models in the medical field. It is sourced from original medical transcriptions and formatted in CSV, featuring instruction-output pairs. Each row includes an instruction, task output, transcription text, description, medical specialty, sample name, and both original and derived keywords. Additional columns provide transcription length, normalized length, and a complexity score. Tasks include identifying medical specialties, summarizing transcriptions, extracting keywords, assessing text complexity, and suggesting follow-up questions. The dataset focuses on the medical domain, supporting diverse instruction-following tasks in healthcare.\n","metadata":{}},{"cell_type":"markdown","source":"### Load Dataset","metadata":{}},{"cell_type":"code","source":"import pyarrow.parquet as pq\nimport pandas as pd\n\n# Step 1: Read Parquet file\ndf = pd.read_csv('/kaggle/input/medicaltranscriptions/mtsamples.csv')\n\n# df = pd.read_csv(\"hf://datasets/DataFog/medical-transcription-instruct/datafog-medical-transcription-instruct.csv\")\n\n# # Step 2: Convert table to DataFrame\n# df = table.to_pandas()\n\n# Step 3: Access data\ndf.head()  # Display first few rows of the DataFrame\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n\n# train, test = train_test_split(df, test_size=0.8)\n\n# # Further split the train set to keep exactly 27,000 in the train set\n# train_final, extra_for_test = train_test_split(train, test_size=1677/len(train), random_state=42)\n\n# # Use pd.concat to combine the extra_for_test with the test set\n# test_final = pd.concat([test, extra_for_test])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize an empty list to store the formatted transcriptions and keywords\nformatted_data = []\n\n# Iterate over each row of the DataFrame\nfor index, row in df.iterrows():\n    # Extract messages\n    transcription = row['transcription']\n    keywords = row['keywords']\n#     instructions = row['instruction']\n#     task_output = row['task_output']\n    description = row['description']\n    medical_specialty = row['medical_specialty']\n    \n    # Create the formatted string\n#     formatted_string = f'Transcription:\\n{transcription}\\n\\nKeywords:\\n{keywords}\\n\\nMedical Specialty:{medical_specialty}\\n\\nInstruction:{instructions}\\n\\nOutput:{task_output}'\n    formatted_string = f'Transcription:\\n{transcription}\\n\\nKeywords:\\n{keywords}\\n\\nMedical Specialty:{medical_specialty}\\n\\nDescription: {description}'\n    \n    # Append the formatted string to the list\n    formatted_data.append(formatted_string)\n\n# Print the formatted data\nfor item in formatted_data:\n    print(item)\n    break\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# formatted_data = formatted_data[:200]\nprint(len(formatted_data))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load Model\n\nKerasNLP provides implementations of many popular model architectures. In this, the model is created using GemmaCausalLM, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.\n\nCreate the model using the from_preset method:","metadata":{}},{"cell_type":"code","source":"# import shutil\n\n# # Zip the file so it can be downloaded\n# shutil.make_archive('/kaggle/working/gemma-medtr-2b-v2', 'zip', '/kaggle/working/gemma-medtr-2b-v2')\n\n# # The file will be available for download from the output section of your notebook\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_model = keras_nlp.models.GemmaCausalLM.from_preset('/kaggle/input/gemma/keras/gemma_2b_en/2')\ngemma_model.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The from_preset method instantiates the model from a preset architecture and weights. In the code above, the string \"gemma_2b_en\" specifies the preset architecture â€” a Gemma model with 2 billion parameters.","metadata":{}},{"cell_type":"markdown","source":"### Inference before fine tuning\nIn this section, we will query the model with various prompts to see how it responds.","metadata":{}},{"cell_type":"markdown","source":"#### laparoscopic gastric bypass Prompt\nQuery the model for laparoscopic gastric bypass transcription.","metadata":{}},{"cell_type":"code","source":"prompt = \"tell me something about treating laparoscopic gastric bypass with an example\"\nprint(gemma_model.generate(prompt, max_length=256))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LoRA Fine-tuning","metadata":{}},{"cell_type":"markdown","source":"To get better responses from the model, fine-tune the model with Low Rank Adaptation (LoRA) using the mt samples dataset.\n\nLoRA (Low-Rank Adaptation of Large Language Models) has gained popularity as a lightweight training method that slashes the number of parameters to train. Instead of adjusting every single weight in the model, LoRA adds a smaller set of new weights and focuses training solely on them. This approach speeds up training, saves memory, and generates smaller model sizes (just a few 100 MBs), making them simpler to store and share. To speed up training more, it can also be combined with other training techniques like dreambooth.\n\nHaving a higher rank allows for more detailed adjustments, which can enhance precision but also increases the number of parameters to train. On the other hand, a lower rank reduces computational burden but might result in less precise adaptation.\n\nFor this, a LoRA rank of 4 is utilized. It's recommended to start with a relatively small rank, like 4, 8, or 16, for computational efficiency during experimentation. Train model with rank=4 initially and assess its performance improvement on specific task. As we progress, we can gradually increase the rank in subsequent experiments to observe if it leads to further enhancements in performance.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 8.\ngemma_model.backbone.enable_lora(rank=8)\ngemma_model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Note that enabling LoRA reduces the number of trainable parameters significantly (from 2.5 billion to 1.3 million).","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameters for lora\nWe are defining only rank, rest will be default hyperparameters.\n\nRank (r): 4\nWe opted for a rank of 4 in our decomposition matrices to maintain efficiency gains while still achieving solid performance. Although testing higher ranks, like 8 or 16, showed minimal performance improvements, although it is recommended to sticking with 8 if harware compatibility is there it helps to keep checkpoint sizes manageable without sacrificing too much accuracy.\n\nAlpha (lora_alpha):\nAlpha scales the learned weights. Based on existing [literature](https://arxiv.org/pdf/2308.07317v1.pdf) and recommendations from the original [LoRA paper](https://arxiv.org/abs/2106.09685). Keeping Alpha fixed rather than treating it as a tunable hyperparameter is a common practice in the LLM community and suggested value is 16.\n\nTarget Modules: All Dense Layers\nWhile the original LoRA paper focused on fine-tuning only the \"Q\" and \"V\" attention matrices, subsequent research suggested that targeting [additional layers](https://arxiv.org/pdf/2110.04366.pdf), or even [all layers](https://arxiv.org/abs/2305.14314), could yield better results. [At backend](https://github.com/keras-team/keras-nlp/blob/v0.8.1/keras_nlp/models/backbone.py#L156) code targets the \"query_dense\" and \"value_dense\" layers in the attention layers for enabling LoRA.\n\nBase Learning Rate: 1e-4\nBase learning rate of 1e-4 has become the standard for fine-tuning LLMs with LoRA. Although [some](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2) have encountered occasional training loss instabilities, lowering the learning rate to values like 3e-5 helped them stabilize the process.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\ntorch.cuda.empty_cache()\nnp.asarray(formatted_data).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage).\ngemma_model.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_model.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_model.fit(formatted_data, epochs=4, batch_size=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade kagglehub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_model.save_to_preset(\"./gemma_medtr_2b_v5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kaggle_uri = \"kaggle://harishiker99/gemma_medtr_2b_v5/keras/gemma_medtr_2b_v5\"\nkeras_nlp.upload_preset(kaggle_uri, \"./gemma_medtr_2b_v5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hf_username = \"harishnair04\"\nmodel_variant_name = \"gemma-medtr-2b-v5\"\n\n\nuri = f\"hf://{hf_username}/{model_variant_name}\"\nprint(uri)\n\n\nimport huggingface_hub\nhuggingface_hub.login(token=\"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\",add_to_git_credential=True)\n\nkeras_nlp.upload_preset(uri, \"/kaggle/working/gemma_medtr-2b-v5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference after fine-tuning","metadata":{}},{"cell_type":"markdown","source":"We're training the model on only a small part of the dataset for multiple epoch and with a low LoRA rank setting. If improved responses are required from the fine-tuned model, we might want to try out the following:\n\n- Expanding the size of the dataset used for fine-tuning.\n- Adjusting the LoRA rank to a higher value.\n- Increasing the number of training steps (epochs).\n- Tweaking the hyperparameter values like learning_rate and weight_decay.","metadata":{}},{"cell_type":"code","source":"prompt=\"tell me something about treating laparoscopic gastric bypass with an example\"\nprint(gemma_model.generate(prompt, max_length=256))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}