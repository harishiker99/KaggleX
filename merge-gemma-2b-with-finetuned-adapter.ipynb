{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":205973910,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-10T02:36:37.036773Z","iopub.execute_input":"2024-11-10T02:36:37.037148Z","iopub.status.idle":"2024-11-10T02:36:38.046891Z","shell.execute_reply.started":"2024-11-10T02:36:37.037112Z","shell.execute_reply":"2024-11-10T02:36:38.045684Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/__results__.html\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/__notebook__.ipynb\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/__output__.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/custom.css\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/adapter_model.safetensors\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/adapter_config.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/README.md\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/adapter_model.safetensors\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/trainer_state.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/training_args.bin\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/adapter_config.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/README.md\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/tokenizer.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/tokenizer_config.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/scheduler.pt\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/special_tokens_map.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/optimizer.pt\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/rng_state.pth\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/tokenizer.model\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-4499/added_tokens.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/adapter_model.safetensors\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/trainer_state.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/training_args.bin\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/adapter_config.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/README.md\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/tokenizer.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/tokenizer_config.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/scheduler.pt\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/special_tokens_map.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/optimizer.pt\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/rng_state.pth\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/tokenizer.model\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2/checkpoint-8998/added_tokens.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/wandb/run-20241109_180315-7do8n4wb/run-7do8n4wb.wandb\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/wandb/run-20241109_180315-7do8n4wb/logs/debug.log\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/wandb/run-20241109_180315-7do8n4wb/logs/debug-internal.log\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/wandb/run-20241109_180315-7do8n4wb/files/wandb-summary.json\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/wandb/run-20241109_180315-7do8n4wb/files/config.yaml\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/wandb/run-20241109_180315-7do8n4wb/files/output.log\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/wandb/run-20241109_180315-7do8n4wb/files/requirements.txt\n/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/wandb/run-20241109_180315-7do8n4wb/files/wandb-metadata.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"! pip install accelerate -q\n! pip install -i https://pypi.org/simple/ bitsandbytes -q\n! pip install peft -q\n! pip install trl -q\n! pip install --upgrade huggingface_hub -q\n! pip install git+https://github.com/huggingface/datasets -U -q\n! pip install git+https://github.com/huggingface/transformers -U -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T02:36:38.049029Z","iopub.execute_input":"2024-11-10T02:36:38.050100Z","iopub.status.idle":"2024-11-10T02:39:09.126754Z","shell.execute_reply.started":"2024-11-10T02:36:38.050052Z","shell.execute_reply":"2024-11-10T02:39:09.125401Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\naccess_token_read = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token = access_token_read)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T02:39:09.128578Z","iopub.execute_input":"2024-11-10T02:39:09.129018Z","iopub.status.idle":"2024-11-10T02:39:09.870855Z","shell.execute_reply.started":"2024-11-10T02:39:09.128979Z","shell.execute_reply":"2024-11-10T02:39:09.870077Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"base_model_url = \"google/gemma-2-2b\"\nnew_model_url = \"/kaggle/input/finetune-gemma-with-peft-4-bit-quantized-lora/Gemma-medtr-2b-sft-v2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T02:39:09.872862Z","iopub.execute_input":"2024-11-10T02:39:09.873167Z","iopub.status.idle":"2024-11-10T02:39:09.877341Z","shell.execute_reply.started":"2024-11-10T02:39:09.873135Z","shell.execute_reply":"2024-11-10T02:39:09.876358Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nfrom peft import PeftModel\nimport torch\nfrom trl import setup_chat_format\n\n\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model_url)\n\nbase_model_reload= AutoModelForCausalLM.from_pretrained(\n    base_model_url,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.bfloat16,\n    device_map=\"cpu\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T02:39:09.878389Z","iopub.execute_input":"2024-11-10T02:39:09.878641Z","iopub.status.idle":"2024-11-10T02:43:52.887867Z","shell.execute_reply.started":"2024-11-10T02:39:09.878613Z","shell.execute_reply":"2024-11-10T02:43:52.886868Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae22a0182a3e49eda9eb409147d73967"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e48f1144bce044318d71135d54abe06f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad2186635a54c4797d1ab8653c2019c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634c97d7aa0348d487bd7abde3c16db0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b85e939279c4bcb8372cf2edb54b67c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac72162524354055bd5e28a1dd211244"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50c3149261584b7db04a14690f4d6b08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88453c6e9894475f8ef36c3af35f1e2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"483efe6242e6417698be9a7f6e49e344"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ca8cd9e303e405fb971589cac54691b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87a6a155791243b288ee5497e2875cf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7909b76ef3ea47b083ffab6b9837f9d2"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\nmodel = PeftModel.from_pretrained(base_model_reload, new_model_url)\n\nmodel = model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T02:43:52.889013Z","iopub.execute_input":"2024-11-10T02:43:52.889578Z","iopub.status.idle":"2024-11-10T02:44:59.263604Z","shell.execute_reply.started":"2024-11-10T02:43:52.889544Z","shell.execute_reply":"2024-11-10T02:44:59.262506Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model.save_pretrained(\"Gemma-medtr-2b-sft-v2\")\ntokenizer.save_pretrained(\"Gemma-medtr-2b-sft-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T02:44:59.265499Z","iopub.execute_input":"2024-11-10T02:44:59.265869Z","iopub.status.idle":"2024-11-10T02:45:12.408391Z","shell.execute_reply.started":"2024-11-10T02:44:59.265815Z","shell.execute_reply":"2024-11-10T02:45:12.407375Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"('Gemma-medtr-2b-sft-v2/tokenizer_config.json',\n 'Gemma-medtr-2b-sft-v2/special_tokens_map.json',\n 'Gemma-medtr-2b-sft-v2/tokenizer.model',\n 'Gemma-medtr-2b-sft-v2/added_tokens.json',\n 'Gemma-medtr-2b-sft-v2/tokenizer.json')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model.push_to_hub(\"Gemma-medtr-2b-sft-v2\", use_temp_dir=False)\ntokenizer.push_to_hub(\"Gemma-medtr-2b-sft-v2\", use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T02:45:12.409555Z","iopub.execute_input":"2024-11-10T02:45:12.409908Z","iopub.status.idle":"2024-11-10T02:48:14.448728Z","shell.execute_reply.started":"2024-11-10T02:45:12.409875Z","shell.execute_reply":"2024-11-10T02:48:14.447725Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5ec189c974648db8f6963d2d2983078"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3972c3b5707f4a1b8bea6d5571f259ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9806feff372a4566bf8e34048f223a02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2035ee7e134342b3a2b0e61ea6a3994a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c55a511adf74e78aacea560fb305d0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9173141bd93544ff92bab2894400d102"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bda22fc048f34f959bb84d40dd98777b"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/harishnair04/Gemma-medtr-2b-sft-v2/commit/8d11bfd8551c60a73e63525b8f4554bbf44169d8', commit_message='Upload tokenizer', commit_description='', oid='8d11bfd8551c60a73e63525b8f4554bbf44169d8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/harishnair04/Gemma-medtr-2b-sft-v2', endpoint='https://huggingface.co', repo_type='model', repo_id='harishnair04/Gemma-medtr-2b-sft-v2'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# # Load model directly\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# tokenizer = AutoTokenizer.from_pretrained(\"harishnair04/Gemma-medtr-2b-sft-v1\")\n# model = AutoModelForCausalLM.from_pretrained(\"harishnair04/Gemma-medtr-2b-sft-v1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T02:48:15.111993Z","iopub.status.idle":"2024-11-10T02:48:15.112347Z","shell.execute_reply.started":"2024-11-10T02:48:15.112172Z","shell.execute_reply":"2024-11-10T02:48:15.112190Z"}},"outputs":[],"execution_count":null}]}